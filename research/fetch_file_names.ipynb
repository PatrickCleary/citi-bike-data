{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url: str = os.environ.get(\"URL\")\n",
    "key: str = os.environ.get(\"ANON_KEY\")\n",
    "supabase: Client = create_client(url, \"sb_secret__hURMlWV0e8lHlKreudFAA_UBScLp6U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://s3.amazonaws.com/tripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.raise_for_status()  # Ensure we notice bad responses\n",
    "soup = BeautifulSoup(response.text, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_citibike_xml_to_df(xml_content):\n",
    "    \"\"\"\n",
    "    Parse the Citibike S3 bucket XML listing into a pandas DataFrame\n",
    "\n",
    "    Args:\n",
    "        xml_content (str): The XML content from the S3 bucket listing\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns 'filename', 'last_modified', 'size_bytes'\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the XML\n",
    "    soup = BeautifulSoup(xml_content, \"xml\")\n",
    "\n",
    "    # Find all Contents elements\n",
    "    contents = soup.find_all(\"Contents\")\n",
    "\n",
    "    # Extract data from each file\n",
    "    file_data = []\n",
    "\n",
    "    for content in contents:\n",
    "        # Get the filename (Key)\n",
    "        key = content.find(\"Key\")\n",
    "        filename = key.text if key else None\n",
    "\n",
    "        # Get the last modified date\n",
    "        last_modified = content.find(\"LastModified\")\n",
    "        last_modified_date = last_modified.text if last_modified else None\n",
    "\n",
    "        # Get the file size\n",
    "        size = content.find(\"Size\")\n",
    "        size_bytes = int(size.text) if size else None\n",
    "\n",
    "        # Add to our data list\n",
    "        if filename:  # Only add if we have a filename\n",
    "            file_data.append(\n",
    "                {\n",
    "                    \"file_name\": filename,\n",
    "                    \"last_modified\": last_modified_date,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(file_data)\n",
    "\n",
    "    # Convert last_modified to datetime\n",
    "    if not df.empty and \"last_modified\" in df.columns:\n",
    "        df[\"last_modified\"] = pd.to_datetime(df[\"last_modified\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_new_files(live_files, prev_files):\n",
    "    # Method 1: Using merge with indicator\n",
    "    merged = live_files.merge(\n",
    "        prev_files,\n",
    "        on=\"file_name\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_new\", \"_old\"),\n",
    "        indicator=True,\n",
    "    )\n",
    "\n",
    "    # Filter for rows that either:\n",
    "    # 1) Don't appear in old_df (_merge == 'left_only')\n",
    "    # 2) Appear in old_df but have newer last_modified date\n",
    "    filtered_df = merged[\n",
    "        (merged[\"_merge\"] == \"left_only\")  # Not in old_df\n",
    "        | (\n",
    "            merged[\"last_modified_new\"] > merged[\"last_modified_old\"]\n",
    "        )  # Newer modification date\n",
    "    ][[\"file_name\", \"last_modified_new\"]].rename(\n",
    "        columns={\"last_modified_new\": \"last_modified\"}\n",
    "    )\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_processed_files():\n",
    "    result = supabase.table(\"processed_files\").select(\"*\").execute()\n",
    "    prev_files = pd.DataFrame(result.data)\n",
    "    return prev_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_files():\n",
    "    response = requests.get(\"https://s3.amazonaws.com/tripdata\")\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    current_files = parse_citibike_xml_to_df(response.text)\n",
    "    return current_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unprocessed_files():\n",
    "    # Previously processed files which we have in the database.\n",
    "    prev_files = get_prev_processed_files()\n",
    "    # Files live on the website.\n",
    "    live_files = get_live_files()\n",
    "    # Get new files which have not been processed. The difference between the two.\n",
    "    new_files = get_new_files(live_files, prev_files)\n",
    "    new_files[\"locale\"] = new_files[\"file_name\"].apply(\n",
    "        lambda x: \"JC\" if x.startswith(\"JC\") else \"NYC\"\n",
    "    )\n",
    "    return new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = get_unprocessed_files()\n",
    "new_files.to_csv(\"new_files_test_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileDownloader:\n",
    "    def __init__(self, base_url: str = \"https://s3.amazonaws.com/tripdata/\"):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_unprocessed_files(self):\n",
    "        \"\"\"\n",
    "        Get files which have not been processed yet.F\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Previously processed files which we have in the database.\n",
    "        prev_files = get_prev_processed_files()\n",
    "        # Files live on the website.\n",
    "        live_files = get_live_files()\n",
    "        # Get new files which have not been processed. The difference between the two.\n",
    "        new_files = get_new_files(live_files, prev_files)\n",
    "        new_files[\"locale\"] = new_files[\"file_name\"].apply(\n",
    "            lambda x: \"JC\" if x.startswith(\"JC\") else \"NYC\"\n",
    "        )\n",
    "        return new_files\n",
    "\n",
    "    def get_prev_processed_files(self):\n",
    "        \"\"\"Get records from database. These are the files we have already processed including last modified date from the citibike website.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        result = supabase.table(\"processed_files\").select(\"*\").execute()\n",
    "        prev_files = pd.DataFrame(result.data)\n",
    "        return prev_files\n",
    "\n",
    "    def get_live_files(self):\n",
    "        \"\"\"Get the current files from the citibike website.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        response = requests.get(\"https://s3.amazonaws.com/tripdata\")\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "        current_files = self.parse_citibike_xml_to_df(response.text)\n",
    "        return current_files\n",
    "\n",
    "    def get_new_files(self, live_files, prev_files):\n",
    "        # Method 1: Using merge with indicator\n",
    "        merged = live_files.merge(\n",
    "            prev_files,\n",
    "            on=\"file_name\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"_new\", \"_old\"),\n",
    "            indicator=True,\n",
    "        )\n",
    "\n",
    "        # Filter for rows that either:\n",
    "        # 1) Don't appear in old_df (_merge == 'left_only')\n",
    "        # 2) Appear in old_df but have newer last_modified date\n",
    "        filtered_df = merged[\n",
    "            (merged[\"_merge\"] == \"left_only\")  # Not in old_df\n",
    "            | (\n",
    "                merged[\"last_modified_new\"] > merged[\"last_modified_old\"]\n",
    "            )  # Newer modification date\n",
    "        ][[\"file_name\", \"last_modified_new\"]].rename(\n",
    "            columns={\"last_modified_new\": \"last_modified\"}\n",
    "        )\n",
    "        return filtered_df\n",
    "\n",
    "    def parse_citibike_xml_to_df(self, xml_content):\n",
    "        \"\"\"\n",
    "        Parse the Citibike S3 bucket XML listing into a pandas DataFrame\n",
    "\n",
    "        Args:\n",
    "            xml_content (str): The XML content from the S3 bucket listing\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with columns 'filename', 'last_modified', 'size_bytes'\n",
    "        \"\"\"\n",
    "\n",
    "        # Parse the XML\n",
    "        soup = BeautifulSoup(xml_content, \"xml\")\n",
    "\n",
    "        # Find all Contents elements\n",
    "        contents = soup.find_all(\"Contents\")\n",
    "\n",
    "        # Extract data from each file\n",
    "        file_data = []\n",
    "\n",
    "        for content in contents:\n",
    "            # Get the filename (Key)\n",
    "            key = content.find(\"Key\")\n",
    "            filename = key.text if key else None\n",
    "\n",
    "            # Get the last modified date\n",
    "            last_modified = content.find(\"LastModified\")\n",
    "            last_modified_date = last_modified.text if last_modified else None\n",
    "\n",
    "            # Get the file size\n",
    "            size = content.find(\"Size\")\n",
    "            size_bytes = int(size.text) if size else None\n",
    "\n",
    "            # Add to our data list\n",
    "            if filename:  # Only add if we have a filename\n",
    "                file_data.append(\n",
    "                    {\n",
    "                        \"file_name\": filename,\n",
    "                        \"last_modified\": last_modified_date,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(file_data)\n",
    "\n",
    "        # Convert last_modified to datetime\n",
    "        if not df.empty and \"last_modified\" in df.columns:\n",
    "            df[\"last_modified\"] = pd.to_datetime(df[\"last_modified\"])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-citibike-tripdata.zip</td>\n",
       "      <td>2024-02-22 14:26:20+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-citibike-tripdata.zip</td>\n",
       "      <td>2024-02-22 14:26:20+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-citibike-tripdata.zip</td>\n",
       "      <td>2024-02-22 14:26:20+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-citibike-tripdata.zip</td>\n",
       "      <td>2024-02-22 14:26:20+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-citibike-tripdata.zip</td>\n",
       "      <td>2024-02-22 14:26:20+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>JC-202505-citibike-tripdata.csv.zip</td>\n",
       "      <td>2025-06-04 15:06:05+00:00</td>\n",
       "      <td>JC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>JC-202506-citibike-tripdata.csv.zip</td>\n",
       "      <td>2025-07-06 22:27:07+00:00</td>\n",
       "      <td>JC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>JC-202507-citibike-tripdata.csv.zip</td>\n",
       "      <td>2025-08-05 17:54:55+00:00</td>\n",
       "      <td>JC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>JC-202508-citibike-tripdata.csv.zip</td>\n",
       "      <td>2025-09-04 19:35:20+00:00</td>\n",
       "      <td>JC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>index.html</td>\n",
       "      <td>2017-01-18 22:23:41+00:00</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               file_name             last_modified locale\n",
       "0             2013-citibike-tripdata.zip 2024-02-22 14:26:20+00:00    NYC\n",
       "1             2014-citibike-tripdata.zip 2024-02-22 14:26:20+00:00    NYC\n",
       "2             2015-citibike-tripdata.zip 2024-02-22 14:26:20+00:00    NYC\n",
       "3             2016-citibike-tripdata.zip 2024-02-22 14:26:20+00:00    NYC\n",
       "4             2017-citibike-tripdata.zip 2024-02-22 14:26:20+00:00    NYC\n",
       "..                                   ...                       ...    ...\n",
       "147  JC-202505-citibike-tripdata.csv.zip 2025-06-04 15:06:05+00:00     JC\n",
       "148  JC-202506-citibike-tripdata.csv.zip 2025-07-06 22:27:07+00:00     JC\n",
       "149  JC-202507-citibike-tripdata.csv.zip 2025-08-05 17:54:55+00:00     JC\n",
       "150  JC-202508-citibike-tripdata.csv.zip 2025-09-04 19:35:20+00:00     JC\n",
       "151                           index.html 2017-01-18 22:23:41+00:00    NYC\n",
       "\n",
       "[145 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_downloader = FileDownloader()\n",
    "file_downloader.get_unprocessed_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citibike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
