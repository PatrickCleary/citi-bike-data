{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h3\n",
    "import hashlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion planning\n",
    "\n",
    "\n",
    "We want to convert the zip files of CSVs from the citi bike data [website](https://s3.amazonaws.com/tripdata/index.html) into a data format we can work with.\n",
    "\n",
    "Big picture the plan is to take the CSVs of individual rides and upload each ride as an entry to the `ride_data` database table.\n",
    "\n",
    "There are a few reasons to reshape the trip data before aggregating by month. The biggest reason is that the file structure for the data dumps is not consistent. So if we pulled down data and directly converted to aggregated monthly formats it would mean the logic for aggregating and the logic for extracting data would be tightly coupled. AKA if we want to change aggregation logic we would need to update the code for each file structure for data.\n",
    "\n",
    "1. The file structure is inconsistent across years/months. Before 2024 all data is in one file\n",
    "2. The `ride_id` field is not present until a certain year.\n",
    "3. We want to easily be able to add more cities.\n",
    "4. When there are multiple files for a single month we can process each one individually and send to the backend without having to load all of them each time. So our system doesn't need to **understand** the file structure. It can just load all the trips in the format we want then process them. \n",
    "\n",
    "\n",
    "This table will have 5 fields:\n",
    "\n",
    "`id`: int incremental unique ID\n",
    "\n",
    "`ride_id`: A unique ID for each ride\n",
    "\n",
    "`locale`: Jersey or NYC for now\n",
    "\n",
    "`start_date`: The starting time for the ride.\n",
    "\n",
    "`created_at` timestamp for creation. \n",
    "\n",
    "\n",
    "\n",
    "A few solutions:\n",
    "\n",
    "1. Data before a certain year does not contain the ride_id field. We will create it for those dates by combining and hashing the `start_time` and the `bike_id` since that should be unique.\n",
    "2. Duplicated trips across files. If they are from recent years they will have `ride_id` which will dedupe. Otherwise our generated `ride_id` will hopefully suffice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "1. Hit index page https://s3.amazonaws.com/tripdata/index.html\n",
    "\n",
    "2. Get all file names/last modified dates. Compare against db to see if anything requires update.\n",
    "\n",
    "3. For any file which has been modified or is new, run ingestion.\n",
    "\n",
    "4. Download zip file, iterate through each sub-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_column_match(df_columns, possible_names):\n",
    "    \"\"\"Find matching column name from possibilities, case-insensitive\"\"\"\n",
    "    df_columns_lower = [col.lower() for col in df_columns]\n",
    "    for possible_name in possible_names:\n",
    "        if possible_name.lower() in df_columns_lower:\n",
    "            # Return the original column name (with original case)\n",
    "            original_idx = df_columns_lower.index(possible_name.lower())\n",
    "            return df_columns[original_idx]\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_all_csvs_from_zip_url(zip_url, locale):\n",
    "    \"\"\"\n",
    "    Download ZIP file from URL and process ALL CSV files found in any folder/subfolder.\n",
    "\n",
    "    Parameters:\n",
    "    zip_url (str): URL to the ZIP file\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary where keys are CSV filenames and values are processed DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Download ZIP file into memory\n",
    "        print(f\"Downloading ZIP file from: {zip_url}\")\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Create a BytesIO object from the downloaded content\n",
    "        zip_data = io.BytesIO(response.content)\n",
    "\n",
    "        return process_all_csvs_from_zip_data(zip_data, locale)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Error downloading ZIP file: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing ZIP file: {str(e)}\")\n",
    "\n",
    "\n",
    "def process_all_csvs_from_zip_data(zip_data, locale):\n",
    "    \"\"\"\n",
    "    Process all CSV files from ZIP data (works with both URL and local files).\n",
    "\n",
    "    Parameters:\n",
    "    zip_data (io.BytesIO): ZIP file data\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary where keys are CSV filenames and values are processed DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    processed_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_data, \"r\") as zip_ref:\n",
    "            # Get ALL files in ZIP (including subfolders)\n",
    "            all_files = zip_ref.namelist()\n",
    "\n",
    "            # Filter for CSV files (case insensitive) and exclude system/metadata files\n",
    "            csv_files = []\n",
    "            for f in all_files:\n",
    "                # Skip directories\n",
    "                if f.endswith(\"/\"):\n",
    "                    continue\n",
    "                # Skip macOS metadata files\n",
    "                if \"__MACOSX\" in f or f.startswith(\"._\"):\n",
    "                    continue\n",
    "                # Skip Windows/Linux hidden files\n",
    "                if \"/.DS_Store\" in f or f.endswith(\".DS_Store\"):\n",
    "                    continue\n",
    "                # Skip other common system files\n",
    "                if f.endswith(\".thumbs.db\") or f.endswith(\"Thumbs.db\"):\n",
    "                    continue\n",
    "                # Keep only CSV files\n",
    "                if f.lower().endswith(\".csv\"):\n",
    "                    csv_files.append(f)\n",
    "\n",
    "            if not csv_files:\n",
    "                raise ValueError(\"No CSV files found in ZIP archive\")\n",
    "\n",
    "            print(f\"Found {len(csv_files)} CSV file(s) in ZIP archive:\")\n",
    "            for csv_file in csv_files:\n",
    "                print(f\"  - {csv_file}\")\n",
    "\n",
    "            # Process each CSV file\n",
    "            for csv_file_path in csv_files:\n",
    "                try:\n",
    "                    print(f\"\\nProcessing: {csv_file_path}\")\n",
    "\n",
    "                    # Read CSV directly from ZIP\n",
    "                    with zip_ref.open(csv_file_path) as csv_file:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "\n",
    "                    # Process the DataFrame based on which format it uses.\n",
    "                    if \"ride_id\" in df.columns:\n",
    "                        print(\"here\")\n",
    "                        processed_df = process_dataframe(df, locale)\n",
    "                    else:\n",
    "                        print(\"239821h\")\n",
    "                        processed_df = process_dataframe_old_format(df, locale)\n",
    "\n",
    "                    # Use just the filename (without path) as key\n",
    "                    filename_only = os.path.basename(csv_file_path)\n",
    "\n",
    "                    # Handle duplicate filenames by adding folder info\n",
    "                    if filename_only in results:\n",
    "                        # Create unique key with folder path\n",
    "                        folder_path = os.path.dirname(csv_file_path)\n",
    "                        unique_key = (\n",
    "                            f\"{folder_path}/{filename_only}\"\n",
    "                            if folder_path\n",
    "                            else filename_only\n",
    "                        )\n",
    "                        results[unique_key] = processed_df\n",
    "                    else:\n",
    "                        results[filename_only] = processed_df\n",
    "\n",
    "                    processed_count += 1\n",
    "                    print(f\"  ✓ Successfully processed {len(processed_df)} rows\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Failed to process {csv_file_path}: {str(e)}\"\n",
    "                    print(f\"  ✗ {error_msg}\")\n",
    "                    failed_files.append((csv_file_path, str(e)))\n",
    "                    continue\n",
    "\n",
    "            # Summary\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"PROCESSING SUMMARY:\")\n",
    "            print(f\"Successfully processed: {processed_count} files\")\n",
    "            print(f\"Failed: {len(failed_files)} files\")\n",
    "\n",
    "            if failed_files:\n",
    "                print(f\"\\nFailed files:\")\n",
    "                for failed_file, error in failed_files:\n",
    "                    print(f\"  - {failed_file}: {error}\")\n",
    "\n",
    "            if processed_count == 0:\n",
    "                raise ValueError(\"No CSV files could be processed successfully\")\n",
    "\n",
    "            return results\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"File is not a valid ZIP archive, skipping.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error processing ZIP file: {str(e)}\")\n",
    "def process_dataframe(df, locale):\n",
    "    \"\"\"\n",
    "    Process DataFrame to create the desired output format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    required_columns = [\n",
    "        \"ride_id\",\n",
    "        \"started_at\",\n",
    "        \"start_lat\",\n",
    "        \"start_lng\",\n",
    "        \"end_lat\",\n",
    "        \"end_lng\",\n",
    "    ]\n",
    "\n",
    "    # Check if all required columns exist\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Create new DataFrame with only the needed columns\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Copy ride_id as-is\n",
    "    result_df[\"ride_id\"] = df[\"ride_id\"]\n",
    "\n",
    "    # Convert started_at to date only (remove time component)\n",
    "    result_df[\"start_date\"] = pd.to_datetime(df[\"started_at\"]).dt.date\n",
    "\n",
    "    # Set local to constant \"JC\"\n",
    "    result_df[\"locale\"] = locale\n",
    "\n",
    "    # Copy latitude and longitude\n",
    "    result_df[\"start_lat\"] = df[\"start_lat\"]\n",
    "    result_df[\"start_lng\"] = df[\"start_lng\"]\n",
    "    result_df[\"end_lat\"] = df[\"end_lat\"]\n",
    "    result_df[\"end_lng\"] = df[\"end_lng\"]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Create hashed ride_id from starttime and bikeid combination\n",
    "def create_ride_id_hash(row):\n",
    "    # Convert both values to strings and concatenate\n",
    "    combined_string = f\"{row['start_time']}_{row['bike_id']}\"\n",
    "    # Create SHA256 hash and take first 16 characters for shorter ID\n",
    "    hash_object = hashlib.sha256(combined_string.encode())\n",
    "    return hash_object.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def process_dataframe_old_format(df, locale):\n",
    "    \"\"\"\n",
    "    Process DataFrame to create the desired output format.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "    locale: Locale identifier\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Processed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Define column mappings with multiple possible names (all lowercase for comparison)\n",
    "    column_mappings = {\n",
    "        \"bikeid\": [\"bikeid\", \"bike_id\", \"bike id\"],\n",
    "        \"starttime\": [\"starttime\", \"start time\", \"start_time\"],\n",
    "        \"start_station_latitude\": [\n",
    "            \"start station latitude\",\n",
    "            \"start_station_latitude\",\n",
    "            \"start station lat\",\n",
    "            \"start_lat\",\n",
    "        ],\n",
    "        \"start_station_longitude\": [\n",
    "            \"start station longitude\",\n",
    "            \"start_station_longitude\",\n",
    "            \"start station lng\",\n",
    "            \"start station lon\",\n",
    "            \"start_lng\",\n",
    "            \"start_lon\",\n",
    "        ],\n",
    "        \"end_station_latitude\": [\n",
    "            \"end station latitude\",\n",
    "            \"end_station_latitude\",\n",
    "            \"end station lat\",\n",
    "            \"end_lat\",\n",
    "        ],\n",
    "        \"end_station_longitude\": [\n",
    "            \"end station longitude\",\n",
    "            \"end_station_longitude\",\n",
    "            \"end station lng\",\n",
    "            \"end station lon\",\n",
    "            \"end_lng\",\n",
    "            \"end_lon\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Find actual column names in the DataFrame\n",
    "    actual_columns = {}\n",
    "    missing_columns = []\n",
    "\n",
    "    for standard_name, possible_names in column_mappings.items():\n",
    "        matched_column = find_column_match(df.columns.tolist(), possible_names)\n",
    "        if matched_column:\n",
    "            actual_columns[standard_name] = matched_column\n",
    "        else:\n",
    "            missing_columns.append(\n",
    "                f\"{standard_name} (tried: {', '.join(possible_names)})\"\n",
    "            )\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns: {missing_columns}. Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # Create new DataFrame with only the needed columns\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Generate unique ride_id using hash of starttime and bikeid\n",
    "    # Create a temporary series for the hash function\n",
    "    temp_df = pd.DataFrame(\n",
    "        {\n",
    "            \"start_time\": df[actual_columns[\"starttime\"]],\n",
    "            \"bike_id\": df[actual_columns[\"bikeid\"]],\n",
    "        }\n",
    "    )\n",
    "    result_df[\"ride_id\"] = temp_df.apply(create_ride_id_hash, axis=1)\n",
    "\n",
    "    # Convert started_at to date only (remove time component)\n",
    "    result_df[\"start_date\"] = pd.to_datetime(df[actual_columns[\"starttime\"]]).dt.date\n",
    "\n",
    "    result_df[\"locale\"] = locale\n",
    "\n",
    "    # Copy latitude and longitude using the matched column names\n",
    "    result_df[\"start_lat\"] = df[actual_columns[\"start_station_latitude\"]]\n",
    "    result_df[\"start_lng\"] = df[actual_columns[\"start_station_longitude\"]]\n",
    "    result_df[\"end_lat\"] = df[actual_columns[\"end_station_latitude\"]]\n",
    "    result_df[\"end_lng\"] = df[actual_columns[\"end_station_longitude\"]]\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def download_and_save_zip(zip_url, local_path):\n",
    "    \"\"\"\n",
    "    Download ZIP file and save locally.\n",
    "\n",
    "    Parameters:\n",
    "    zip_url (str): URL to download\n",
    "    local_path (str): Local path to save the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading ZIP file to: {local_path}\")\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save file\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"ZIP file saved successfully to: {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"Error downloading ZIP file: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error saving ZIP file: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_h3_latlng_to_cell(df_by_file, resolution=9) -> dict[str, pd.DataFrame]:\n",
    "    output_obj = {}\n",
    "    for key in df_by_file.keys():\n",
    "        print(f\"[{key}] entries: {df_by_file[key].shape[0]}\")\n",
    "        resolution = 9\n",
    "        df_in_loop = df_by_file[key].copy()\n",
    "        df_in_loop[\"h3_cell_start\"] = df_in_loop.apply(\n",
    "            lambda row: (\n",
    "                h3.latlng_to_cell(row[\"start_lat\"], row[\"start_lng\"], resolution)\n",
    "                if pd.notnull(row[\"start_lat\"]) and pd.notnull(row[\"start_lng\"])\n",
    "                else None\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        df_in_loop[\"h3_cell_end\"] = df_in_loop.apply(\n",
    "            lambda row: (\n",
    "                h3.latlng_to_cell(row[\"end_lat\"], row[\"end_lng\"], resolution)\n",
    "                if pd.notnull(row[\"end_lat\"]) and pd.notnull(row[\"end_lng\"])\n",
    "                else None\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        df_in_loop.drop(\n",
    "            columns=[\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"], inplace=True\n",
    "        )\n",
    "        # Remove duplicates. These are infrequent and probably just bad data.\n",
    "        dupes = df_in_loop.duplicated(subset=[\"ride_id\"])\n",
    "        print(f\"  - Removing {dupes.sum()} duplicate ride_id entries\")\n",
    "        df_in_loop = df_in_loop[~dupes]\n",
    "\n",
    "        output_obj[key] = df_in_loop\n",
    "    return output_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "url: str = os.environ.get(\"URL\")\n",
    "key: str = os.environ.get(\"ANON_KEY\")\n",
    "supabase: Client = create_client(url, \"sb_secret__hURMlWV0e8lHlKreudFAA_UBScLp6U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_file_record(file_df_entry):\n",
    "    json_str = file_df_entry.to_json(date_format='iso')\n",
    "    data = json.loads(json_str)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "import time\n",
    "\n",
    "def upload_results_with_guaranteed_retry(\n",
    "    output, batch_size=1000, max_workers=2, max_retries=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload with comprehensive retry logic. Process fails only if retries are exhausted.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Starting upload process with {len(output)} files, max_retries={max_retries}\"\n",
    "    )\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {}\n",
    "\n",
    "        # Submit all jobs with staggered starts\n",
    "        for i, (file_name, df) in enumerate(output.items()):\n",
    "            time.sleep(i * 0.2)  # Stagger submissions\n",
    "            future = executor.submit(\n",
    "                upload_file_with_retry, file_name, df, batch_size, max_retries\n",
    "            )\n",
    "            future_to_file[future] = file_name\n",
    "\n",
    "        # Wait for all to complete\n",
    "        failed_files = []\n",
    "        for future in as_completed(future_to_file):\n",
    "            file_name = future_to_file[future]\n",
    "            try:\n",
    "                future.result()  # This will raise if all retries failed\n",
    "                print(f\"🎉 Successfully completed {file_name}\")\n",
    "            except Exception as e:\n",
    "                failed_files.append(file_name)\n",
    "                print(f\"💥 FINAL FAILURE for {file_name}: {e}\")\n",
    "\n",
    "                # Cancel remaining work\n",
    "                for pending_future in future_to_file:\n",
    "                    if not pending_future.done():\n",
    "                        pending_future.cancel()\n",
    "                        print(f\"⏹️  Cancelled remaining upload\")\n",
    "                break\n",
    "\n",
    "        if failed_files:\n",
    "            raise RuntimeError(f\"Upload process failed. Failed files: {failed_files}\")\n",
    "\n",
    "    print(\"✅ All uploads completed successfully!\")\n",
    "\n",
    "\n",
    "def upload_file_with_retry(file_name: str, df, batch_size: int, max_retries: int):\n",
    "    \"\"\"Upload a single file with comprehensive retry logic for each batch\"\"\"\n",
    "    print(f\"📁 Processing {file_name} with {len(df)} records\")\n",
    "    \n",
    "    json_str = df.to_json(orient=\"records\", date_format=\"iso\")\n",
    "    data = json.loads(json_str)\n",
    "    total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i : i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "\n",
    "        # Retry this specific batch\n",
    "        success = upload_batch_with_retry(\n",
    "            batch, file_name, batch_num, total_batches, max_retries\n",
    "        )\n",
    "\n",
    "        if not success:\n",
    "            raise RuntimeError(f\"Batch {batch_num} failed after {max_retries} retries\")\n",
    "\n",
    "        # Small delay between batches within same file\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"✅ Completed all batches for {file_name}\")\n",
    "\n",
    "\n",
    "def upload_batch_with_retry(\n",
    "    batch, file_name: str, batch_num: int, total_batches: int, max_retries: int\n",
    ") -> bool:\n",
    "    \"\"\"Retry a single batch with exponential backoff\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            log_connection_info()\n",
    "            result = (\n",
    "                supabase.table(\"ride_data\")\n",
    "                .upsert(batch, on_conflict=\"ride_id,locale,start_date\")\n",
    "                .execute()\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"[{file_name}] ✅ Batch {batch_num}/{total_batches}: {len(batch)} records\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "\n",
    "            wait_time = calculate_backoff(attempt)\n",
    "            remaining_attempts = max_retries - attempt - 1\n",
    "\n",
    "            print(\n",
    "                f\"[{file_name}] ⚠️ Batch {batch_num} failed (attempt {attempt + 1}/{max_retries}): {e}\"\n",
    "            )\n",
    "\n",
    "            if remaining_attempts > 0:\n",
    "                print(\n",
    "                    f\"[{file_name}] ⏳ Retrying in {wait_time:.1f}s... ({remaining_attempts} attempts left)\"\n",
    "                )\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[{file_name}] 💥 Batch {batch_num} exhausted all {max_retries} retries\"\n",
    "                )\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_backoff(\n",
    "    attempt: int, base_delay: float = 1.0, max_delay: float = 60.0\n",
    ") -> float:\n",
    "    \"\"\"Calculate exponential backoff with jitter\"\"\"\n",
    "    delay = min(base_delay * (2**attempt), max_delay)\n",
    "    # Add jitter to avoid thundering herd\n",
    "    jitter = random.uniform(0.1, 0.3) * delay\n",
    "    return delay + jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect: connection to server at \"aws-1-us-east-1.pooler.supabase.com\" (3.227.209.82), port 5432 failed: FATAL:  Authentication error, reason: \"Authentication query failed: %DBConnection.ConnectionError{message: \\\"connection not available and request was dropped from queue after 10000ms. This means requests are coming in and your connection pool cannot serve them fast enough. You can address this by:\\\\n\\\\n  1. Ensuring your database is available and that you can connect to it\\\\n  2. Tracking down slow queries and making sure they are running fast enough\\\\n  3. Increasing the pool_size (although this increases resource consumption)\\\\n  4. Allowing requests to wait longer by increasing :queue_target and :queue_interval\\\\n\\\\nSee DBConnection.start_link/2 for more information\\\\n\\\", severity: :error, reason: :queue_timeout}\"\n",
      "connection to server at \"aws-1-us-east-1.pooler.supabase.com\" (3.227.209.82), port 5432 failed: FATAL:  Authentication error, reason: \"Authentication query failed: %DBConnection.ConnectionError{message: \\\"connection not available and request was dropped from queue after 10000ms. This means requests are coming in and your connection pool cannot serve them fast enough. You can address this by:\\\\n\\\\n  1. Ensuring your database is available and that you can connect to it\\\\n  2. Tracking down slow queries and making sure they are running fast enough\\\\n  3. Increasing the pool_size (although this increases resource consumption)\\\\n  4. Allowing requests to wait longer by increasing :queue_target and :queue_interval\\\\n\\\\nSee DBConnection.start_link/2 for more information\\\\n\\\", severity: :error, reason: :queue_timeout}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch variables\n",
    "USER = os.getenv(\"user\")\n",
    "PASSWORD = os.getenv(\"password\")\n",
    "HOST = os.getenv(\"host\")\n",
    "PORT = os.getenv(\"port\")\n",
    "DBNAME = os.getenv(\"dbname\")\n",
    "\n",
    "password=\"T5Sxsx9Lt18Kb4EW\"\n",
    "user=\"postgres.kevndteqglsoslznrntz\" \n",
    "host=\"aws-1-us-east-1.pooler.supabase.com\"\n",
    "port=5432\n",
    "dbname=\"postgres\"\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        dbname=dbname\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    # Create a cursor to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Example query\n",
    "    cursor.execute(\"SELECT NOW();\")\n",
    "    result = cursor.fetchone()\n",
    "    print(\"Current Time:\", result)\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import psycopg2  # or asyncpg\n",
    "\n",
    "# user='postgres'\n",
    "# password=\"T5Sxsx9Lt18Kb4EW\"\n",
    "# host='db.kevndteqglsoslznrntz.supabase.co'\n",
    "# port=6543\n",
    "# dbname='postgres'\n",
    "user = \"postgres.kevndteqglsoslznrntz\"\n",
    "host = \"aws-1-us-east-1.pooler.supabase.com\"\n",
    "port = 5432\n",
    "dbname = \"postgres\"\n",
    "# Your original code should now work\n",
    "engine = sqlalchemy.create_engine(\n",
    "    f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_df = pd.read_csv(\"./new_files_test.csv\")\n",
    "for _, file in file_df.iterrows():\n",
    "    url = f\"https://s3.amazonaws.com/tripdata/{file['file_name']}\"\n",
    "    df_by_file = process_all_csvs_from_zip_url(url, locale=\"NYC\")\n",
    "    if not df_by_file:\n",
    "        print(f\"No data processed for {file['file_name']}, skipping upload.\")\n",
    "    else:\n",
    "        output = apply_h3_latlng_to_cell(df_by_file)\n",
    "        # for _, file in output.iterrows():\n",
    "        #     file.to_sql(\"ride_data\", connection, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BikeShareProcessor:\n",
    "    def __init__(self, conn_details):\n",
    "        self.conn_details = conn_details\n",
    "        self.create_new_connection()\n",
    "\n",
    "    def reset_connection(self):\n",
    "        \"\"\"Create a new connection\"\"\"\n",
    "        try:\n",
    "            self.conn.close()\n",
    "        except:\n",
    "            pass\n",
    "        self.create_new_connection()\n",
    "        self.connection_created_at = datetime.now()\n",
    "        logger.info(\"Database connection reset successfully\")\n",
    "\n",
    "    def create_new_connection(self):\n",
    "        self.conn = psycopg2.connect(**self.conn_details)\n",
    "\n",
    "    def process_files_df(self, files: pd.DataFrame):\n",
    "        for _, file in files.iterrows():\n",
    "            self.reset_connection()\n",
    "            output = self.fetch_and_process_file(file)\n",
    "            if output:\n",
    "                self.upload_output_obj(output, table_name=\"ride_data\")\n",
    "                self.mark_file_as_processed(file)\n",
    "            else:\n",
    "                print(f\"No data processed for {file['file_name']}, skipping upload.\")\n",
    "\n",
    "    def fetch_and_process_file(self, file: pd.Series):\n",
    "        url = f\"https://s3.amazonaws.com/tripdata/{file['file_name']}\"\n",
    "        df_by_file = process_all_csvs_from_zip_url(url, locale=file[\"locale\"])\n",
    "        if not df_by_file:\n",
    "            print(f\"No data processed for {file['file_name']}, skipping upload.\")\n",
    "        else:\n",
    "            output = apply_h3_latlng_to_cell(df_by_file)\n",
    "        return output\n",
    "\n",
    "    def upload_output_obj(self, output_obj: dict[str, pd.DataFrame], table_name: str):\n",
    "        for key in output_obj.keys():\n",
    "            print(f\"Uploading {key} with {output_obj[key].shape[0]} records\")\n",
    "            self.upload_df(output_obj[key], table_name)\n",
    "\n",
    "    def upload_df(self, df: pd.DataFrame, table_name: str, chunk_size=50000):\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            print(f\"uploading chunk {i/ chunk_size}\")\n",
    "            chunk = df.iloc[i : i + 50000]\n",
    "            self.bulk_insert_with_staging(chunk, table_name)\n",
    "\n",
    "    def bulk_insert_with_staging(self, df: pd.DataFrame, table_name: str):\n",
    "        \"\"\"\n",
    "        High-performance bulk insert using staging table and COPY with UPSERT capability.\n",
    "        Updates existing records and inserts new ones.\n",
    "        \"\"\"\n",
    "        if len(df) == 0:\n",
    "            return {\"inserted\": 0, \"updated\": 0}\n",
    "\n",
    "        logger.info(f\"Bulk upserting {len(df)} records to {table_name}\")\n",
    "\n",
    "        with self.conn.cursor() as cur:\n",
    "            # Create temporary staging table\n",
    "            staging_table = f\"staging_{table_name}_{int(datetime.now().timestamp())}\"\n",
    "\n",
    "            cur.execute(\n",
    "                f\"\"\"\n",
    "                CREATE TEMP TABLE {staging_table} (LIKE {table_name} INCLUDING ALL)\n",
    "                ON COMMIT DROP\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            # Use COPY to load data into staging table (fastest method)\n",
    "            output = StringIO()\n",
    "            df.to_csv(output, sep=\"\\t\", header=False, index=False, na_rep=\"\\\\N\")\n",
    "            output.seek(0)\n",
    "\n",
    "            # Build COPY command with correct column list\n",
    "            columns = \", \".join(df.columns)\n",
    "            copy_sql = f\"COPY {staging_table} ({columns}) FROM STDIN WITH (FORMAT csv, DELIMITER E'\\\\t', NULL '\\\\N')\"\n",
    "\n",
    "            cur.copy_expert(copy_sql, output)\n",
    "\n",
    "            # Get all columns except the conflict resolution columns for the UPDATE SET clause\n",
    "            all_columns = list(df.columns)\n",
    "            conflict_columns = [\n",
    "                \"ride_id\",\n",
    "                \"start_date\",\n",
    "                \"locale\",\n",
    "            ]  # Your unique constraint columnsF\n",
    "            update_columns = [col for col in all_columns if col not in conflict_columns]\n",
    "\n",
    "            # Build the SET clause for updates\n",
    "            set_clause = \", \".join(\n",
    "                [f\"{col} = EXCLUDED.{col}\" for col in update_columns]\n",
    "            )\n",
    "\n",
    "            # Perform UPSERT operation\n",
    "            upsert_sql = f\"\"\"\n",
    "                INSERT INTO {table_name} ({columns})\n",
    "                SELECT {columns} FROM {staging_table}\n",
    "                ON CONFLICT (ride_id, start_date, locale) \n",
    "                DO UPDATE SET {set_clause}\n",
    "            \"\"\"\n",
    "\n",
    "            cur.execute(upsert_sql)\n",
    "\n",
    "            cur.execute(upsert_sql)\n",
    "            total_processed = len(df)\n",
    "\n",
    "            self.conn.commit()\n",
    "\n",
    "            logger.info(\n",
    "                f\"Successfully processed {total_processed} records in {table_name}\"\n",
    "            )\n",
    "\n",
    "            return {\"total_processed\": total_processed}\n",
    "\n",
    "    def mark_file_as_processed(self, file_obj):\n",
    "        print(f\"Marking file {file_obj['file_name']} as completed\")\n",
    "        result = (\n",
    "            supabase.table(\"processed_files\")\n",
    "            .upsert(\n",
    "                get_processed_file_record(file_obj),\n",
    "                on_conflict=\"file_name,locale\",\n",
    "            )\n",
    "            .execute()\n",
    "        )\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202404-citibike-tripdata.zip\n",
      "Found 4 CSV file(s) in ZIP archive:\n",
      "  - 202404-citibike-tripdata_4.csv\n",
      "  - 202404-citibike-tripdata_1.csv\n",
      "  - 202404-citibike-tripdata_2.csv\n",
      "  - 202404-citibike-tripdata_3.csv\n",
      "\n",
      "Processing: 202404-citibike-tripdata_4.csv\n",
      "here\n",
      "  ✓ Successfully processed 217063 rows\n",
      "\n",
      "Processing: 202404-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202404-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202404-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 4 files\n",
      "Failed: 0 files\n",
      "[202404-citibike-tripdata_4.csv] entries: 217063\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202404-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202404-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202404-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202404-citibike-tripdata_4.csv with 217063 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "Uploading 202404-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202404-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202404-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202404-citibike-tripdata.zip as completed\n",
      "data=[{'id': 364, 'created_at': '2025-09-16T18:21:51.528314+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202404-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202405-citibike-tripdata.zip\n",
      "Found 5 CSV file(s) in ZIP archive:\n",
      "  - 202405-citibike-tripdata_3.csv\n",
      "  - 202405-citibike-tripdata_2.csv\n",
      "  - 202405-citibike-tripdata_1.csv\n",
      "  - 202405-citibike-tripdata_5.csv\n",
      "  - 202405-citibike-tripdata_4.csv\n",
      "\n",
      "Processing: 202405-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202405-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202405-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202405-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 133961 rows\n",
      "\n",
      "Processing: 202405-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 5 files\n",
      "Failed: 0 files\n",
      "[202405-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202405-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202405-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202405-citibike-tripdata_5.csv] entries: 133961\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202405-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202405-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202405-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202405-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202405-citibike-tripdata_5.csv with 133961 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "Uploading 202405-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202405-citibike-tripdata.zip as completed\n",
      "data=[{'id': 365, 'created_at': '2025-09-16T18:31:19.068414+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202405-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202406-citibike-tripdata.zip\n",
      "Found 5 CSV file(s) in ZIP archive:\n",
      "  - 202406-citibike-tripdata_5.csv\n",
      "  - 202406-citibike-tripdata_4.csv\n",
      "  - 202406-citibike-tripdata_1.csv\n",
      "  - 202406-citibike-tripdata_3.csv\n",
      "  - 202406-citibike-tripdata_2.csv\n",
      "\n",
      "Processing: 202406-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 783576 rows\n",
      "\n",
      "Processing: 202406-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202406-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202406-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202406-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 5 files\n",
      "Failed: 0 files\n",
      "[202406-citibike-tripdata_5.csv] entries: 783576\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202406-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202406-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202406-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202406-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202406-citibike-tripdata_5.csv with 783576 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "Uploading 202406-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202406-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202406-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202406-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202406-citibike-tripdata.zip as completed\n",
      "data=[{'id': 366, 'created_at': '2025-09-16T18:40:54.969298+00:00', 'last_modified': '2025-07-09T15:30:28+00:00', 'file_name': '202406-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202407-citibike-tripdata.zip\n",
      "Found 5 CSV file(s) in ZIP archive:\n",
      "  - 202407-citibike-tripdata_2.csv\n",
      "  - 202407-citibike-tripdata_3.csv\n",
      "  - 202407-citibike-tripdata_1.csv\n",
      "  - 202407-citibike-tripdata_4.csv\n",
      "  - 202407-citibike-tripdata_5.csv\n",
      "\n",
      "Processing: 202407-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202407-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202407-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202407-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202407-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 722896 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 5 files\n",
      "Failed: 0 files\n",
      "[202407-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202407-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202407-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202407-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202407-citibike-tripdata_5.csv] entries: 722896\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202407-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202407-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202407-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202407-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202407-citibike-tripdata_5.csv with 722896 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "Marking file 202407-citibike-tripdata.zip as completed\n",
      "data=[{'id': 367, 'created_at': '2025-09-16T18:49:44.834216+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202407-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202408-citibike-tripdata.zip\n",
      "Found 5 CSV file(s) in ZIP archive:\n",
      "  - 202408-citibike-tripdata_3.csv\n",
      "  - 202408-citibike-tripdata_2.csv\n",
      "  - 202408-citibike-tripdata_1.csv\n",
      "  - 202408-citibike-tripdata_5.csv\n",
      "  - 202408-citibike-tripdata_4.csv\n",
      "\n",
      "Processing: 202408-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202408-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202408-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202408-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 603575 rows\n",
      "\n",
      "Processing: 202408-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 5 files\n",
      "Failed: 0 files\n",
      "[202408-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202408-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202408-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202408-citibike-tripdata_5.csv] entries: 603575\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202408-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202408-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202408-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202408-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202408-citibike-tripdata_5.csv with 603575 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "Uploading 202408-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202408-citibike-tripdata.zip as completed\n",
      "data=[{'id': 368, 'created_at': '2025-09-16T18:58:26.171123+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202408-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202409-citibike-tripdata.zip\n",
      "Found 5 CSV file(s) in ZIP archive:\n",
      "  - 202409-citibike-tripdata_4.csv\n",
      "  - 202409-citibike-tripdata_5.csv\n",
      "  - 202409-citibike-tripdata_1.csv\n",
      "  - 202409-citibike-tripdata_2.csv\n",
      "  - 202409-citibike-tripdata_3.csv\n",
      "\n",
      "Processing: 202409-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202409-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 997898 rows\n",
      "\n",
      "Processing: 202409-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202409-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202409-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 5 files\n",
      "Failed: 0 files\n",
      "[202409-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202409-citibike-tripdata_5.csv] entries: 997898\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202409-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202409-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202409-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202409-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202409-citibike-tripdata_5.csv with 997898 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202409-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202409-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202409-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202409-citibike-tripdata.zip as completed\n",
      "data=[{'id': 369, 'created_at': '2025-09-16T19:08:26.40015+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202409-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202410-citibike-tripdata.zip\n",
      "Found 6 CSV file(s) in ZIP archive:\n",
      "  - 202410-citibike-tripdata_6.csv\n",
      "  - 202410-citibike-tripdata_4.csv\n",
      "  - 202410-citibike-tripdata_5.csv\n",
      "  - 202410-citibike-tripdata_1.csv\n",
      "  - 202410-citibike-tripdata_2.csv\n",
      "  - 202410-citibike-tripdata_3.csv\n",
      "\n",
      "Processing: 202410-citibike-tripdata_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 150054 rows\n",
      "\n",
      "Processing: 202410-citibike-tripdata_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202410-citibike-tripdata_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202410-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202410-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202410-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 6 files\n",
      "Failed: 0 files\n",
      "[202410-citibike-tripdata_6.csv] entries: 150054\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202410-citibike-tripdata_4.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202410-citibike-tripdata_5.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202410-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202410-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202410-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202410-citibike-tripdata_6.csv with 150054 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "Uploading 202410-citibike-tripdata_4.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202410-citibike-tripdata_5.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202410-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202410-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202410-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202410-citibike-tripdata.zip as completed\n",
      "data=[{'id': 370, 'created_at': '2025-09-16T19:19:03.501309+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202410-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202411-citibike-tripdata.zip\n",
      "Found 4 CSV file(s) in ZIP archive:\n",
      "  - 202411-citibike-tripdata_3.csv\n",
      "  - 202411-citibike-tripdata_2.csv\n",
      "  - 202411-citibike-tripdata_1.csv\n",
      "  - 202411-citibike-tripdata_4.csv\n",
      "\n",
      "Processing: 202411-citibike-tripdata_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202411-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202411-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202411-citibike-tripdata_4.csv\n",
      "here\n",
      "  ✓ Successfully processed 710134 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 4 files\n",
      "Failed: 0 files\n",
      "[202411-citibike-tripdata_3.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202411-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202411-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202411-citibike-tripdata_4.csv] entries: 710134\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202411-citibike-tripdata_3.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202411-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202411-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202411-citibike-tripdata_4.csv with 710134 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "Marking file 202411-citibike-tripdata.zip as completed\n",
      "data=[{'id': 371, 'created_at': '2025-09-16T19:25:08.011239+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202411-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n",
      "Downloading ZIP file from: https://s3.amazonaws.com/tripdata/202412-citibike-tripdata.zip\n",
      "Found 3 CSV file(s) in ZIP archive:\n",
      "  - 202412-citibike-tripdata_1.csv\n",
      "  - 202412-citibike-tripdata_3.csv\n",
      "  - 202412-citibike-tripdata_2.csv\n",
      "\n",
      "Processing: 202412-citibike-tripdata_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "Processing: 202412-citibike-tripdata_3.csv\n",
      "here\n",
      "  ✓ Successfully processed 311171 rows\n",
      "\n",
      "Processing: 202412-citibike-tripdata_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/5vjw3k0j25g828djlrwpl83c0000gn/T/ipykernel_41784/778921.py:101: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "  ✓ Successfully processed 1000000 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "Successfully processed: 3 files\n",
      "Failed: 0 files\n",
      "[202412-citibike-tripdata_1.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202412-citibike-tripdata_3.csv] entries: 311171\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "[202412-citibike-tripdata_2.csv] entries: 1000000\n",
      "  - Removing 0 duplicate ride_id entries\n",
      "Uploading 202412-citibike-tripdata_1.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Uploading 202412-citibike-tripdata_3.csv with 311171 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "Uploading 202412-citibike-tripdata_2.csv with 1000000 records\n",
      "uploading chunk 0.0\n",
      "uploading chunk 1.0\n",
      "uploading chunk 2.0\n",
      "uploading chunk 3.0\n",
      "uploading chunk 4.0\n",
      "uploading chunk 5.0\n",
      "uploading chunk 6.0\n",
      "uploading chunk 7.0\n",
      "uploading chunk 8.0\n",
      "uploading chunk 9.0\n",
      "uploading chunk 10.0\n",
      "uploading chunk 11.0\n",
      "uploading chunk 12.0\n",
      "uploading chunk 13.0\n",
      "uploading chunk 14.0\n",
      "uploading chunk 15.0\n",
      "uploading chunk 16.0\n",
      "uploading chunk 17.0\n",
      "uploading chunk 18.0\n",
      "uploading chunk 19.0\n",
      "Marking file 202412-citibike-tripdata.zip as completed\n",
      "data=[{'id': 374, 'created_at': '2025-09-16T19:28:44.879937+00:00', 'last_modified': '2025-07-03T15:01:20+00:00', 'file_name': '202412-citibike-tripdata.zip', 'locale': 'NYC'}] count=None\n"
     ]
    }
   ],
   "source": [
    "connection_pspg2 = psycopg2.connect(\n",
    "    user=user, password=password, host=host, port=port, dbname=dbname\n",
    ")\n",
    "conn_details = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"host\": host,\n",
    "    \"port\": port,\n",
    "    \"dbname\": dbname,\n",
    "}\n",
    "new_files = pd.read_csv(\"./new_files_test.csv\")\n",
    "processor = BikeShareProcessor(conn_details)\n",
    "processor.process_files_df(new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_50k = df.head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df), 50000):\n",
    "    print(f\"uploading chunk {i}\")\n",
    "    chunk = df.iloc[i : i + 50000]\n",
    "    processor.bulk_insert_with_staging(chunk, \"ride_data\")\n",
    "# processor.bulk_insert_with_staging(rows_50k, \"ride_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.postgresql import insert\n",
    "\n",
    "def upsert_postgresql(df, table_name, engine, conflict_columns=['id']):\n",
    "    \"\"\"Insert with ON CONFLICT DO UPDATE - supports composite keys\"\"\"\n",
    "    df.to_sql(table_name, engine, if_exists='append', index=False, \n",
    "              method=lambda pd_table, conn, keys, data_iter: \n",
    "              upsert_method_pg(pd_table, conn, keys, data_iter, conflict_columns))\n",
    "\n",
    "def upsert_method_pg(pd_table, conn, keys, data_iter, conflict_columns):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    stmt = insert(pd_table.table).values(data)\n",
    "    \n",
    "    # Create update dict excluding ALL conflict columns\n",
    "    update_dict = {c.name: c for c in stmt.excluded if c.name not in conflict_columns}\n",
    "    \n",
    "    stmt = stmt.on_conflict_do_update(\n",
    "        index_elements=conflict_columns,  # Pass list of columns\n",
    "        set_=update_dict\n",
    "    )\n",
    "    conn.execute(stmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_obj):\n",
    "    url = f\"https://s3.amazonaws.com/tripdata/{file_obj['file_name']}\"\n",
    "    df_by_file = process_all_csvs_from_zip_url(url, locale=\"NYC\")\n",
    "    if not df_by_file:\n",
    "        print(f\"No data processed for {file_obj['file_name']}, skipping upload.\")\n",
    "        return\n",
    "    output = apply_h3_latlng_to_cell(df_by_file)\n",
    "\n",
    "\n",
    "    upload_results_with_guaranteed_retry(output, max_workers=3, batch_size=500)\n",
    "    # Update supabase processed_files table\n",
    "    print(f\"Updating processed_files table for {file_obj['file_name']}\")\n",
    "    result = (\n",
    "        supabase.table(\"processed_files\")\n",
    "        .upsert(\n",
    "            get_processed_file_record(file_obj),\n",
    "            on_conflict=\"file_name,locale\",\n",
    "        )\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def log_connection_info():\n",
    "    # Add this before each batch upload\n",
    "    print(f\"Timestamp: {time.time()}\")\n",
    "    # This will help correlate timing with your batch numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_df) -> None:\n",
    "    for index, row in file_df.iterrows():\n",
    "        process_file(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_df = pd.read_csv(\"./new_files_test.csv\")\n",
    "process_files(file_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citibike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
